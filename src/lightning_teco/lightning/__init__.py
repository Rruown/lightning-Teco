from lightning_utilities import module_available

if module_available('pytorch_lightning'):
    from ._pytorch_lighting_api import (_DEVICE,
                                        ReduceOp,
                                        CheckpointIO,
                                        ClusterEnvironment,
                                        MisconfigurationException,
                                        Trainer,
                                        Accelerator,
                                        PrecisionPlugin,
                                        DDPStrategy,
                                        SingleDeviceStrategy,
                                        _WrappingCheckpointIO,
                                        TorchElasticEnvironment,
                                        atomic_save, get_filesystem,
                                        TorchCheckpointIO,
                                        move_data_to_device,
                                        _sync_ddp,
                                        rank_zero_info, rank_zero_warn,
                                        FSDPPrecision,
                                        default_pg_timeout,
                                        _StrategyRegistry,
                                        _move_torchmetrics_to_device,
                                        FSDPStrategy,
                                        _setup_activation_checkpointing,
                                        _has_meta_device_parameters_or_buffers)
elif module_available('lightning'):
    from ._lightning_api import (_DEVICE,
                                 ReduceOp,
                                 CheckpointIO,
                                 ClusterEnvironment,
                                 MisconfigurationException,
                                 Trainer,
                                 Accelerator,
                                 PrecisionPlugin,
                                 DDPStrategy,
                                 SingleDeviceStrategy,
                                 _WrappingCheckpointIO,
                                 TorchElasticEnvironment,
                                 atomic_save, get_filesystem,
                                 TorchCheckpointIO,
                                 move_data_to_device,
                                 _sync_ddp,
                                 rank_zero_info, rank_zero_warn,
                                 FSDPPrecision,
                                 default_pg_timeout,
                                 _StrategyRegistry,
                                 _move_torchmetrics_to_device,
                                 FSDPStrategy,
                                 _setup_activation_checkpointing,
                                 _has_meta_device_parameters_or_buffers)
    raise ModuleNotFoundError(
        "You are missing `lightning` or `pytorch-lightning` package, please install it.")
